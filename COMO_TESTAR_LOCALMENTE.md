# Como Testar Localmente - Guia Pr√°tico

## Resumo Executivo

Voc√™ implementou com sucesso um sistema ass√≠ncrono de job queue para resolver o problema de timeout do n8n. Agora vamos testar localmente antes de fazer deploy em produ√ß√£o.

**Tempo total estimado**: 5-10 minutos

## O Que Mudou

Antes (S√≠ncrono - Problem√°tico):
```
POST /scrape-channels ‚Üí API bloqueia por 5-10 minutos ‚Üí n8n timeout ‚ùå
```

Depois (Ass√≠ncrono - Resolvido):
```
POST /scrape-channels/start ‚Üí API retorna em < 100ms com job_id ‚úÖ
GET /scrape-channels/result/{job_id} ‚Üí Polling sem timeout ‚úÖ
```

## Passo 1: Verificar Docker Compose

Verifique se os containers est√£o rodando:

```bash
docker-compose ps
```

Esperado:
```
NAME              STATUS      PORTS
scrape-th-api     Up          0.0.0.0:8000->8000/tcp
selenium-chrome   Up          0.0.0.0:4444->4444/tcp
```

Se n√£o est√£o rodando, inicie:
```bash
docker-compose up -d
```

## Passo 2: Teste R√°pido com Script

Execute este simples script que faz todo o teste automaticamente:

```bash
python test_job_queue_quick.py
```

Isso vai:
1. ‚úÖ Iniciar um job de scraping
2. ‚úÖ Fazer polling autom√°tico a cada 5 segundos
3. ‚úÖ Exibir progresso em tempo real
4. ‚úÖ Aguardar at√© conclus√£o (5-10 minutos)
5. ‚úÖ Exibir resultado final

**Esperado**:
```
============================================================
TESTE DO SISTEMA ASS√çNCRONO - JOB QUEUE
============================================================

1Ô∏è‚É£  Iniciando job de scraping...
   Job ID: a24992f9-a314-480d-a691-07f0f58f93e1
   Status: pending
   Message: Job enfileirado com sucesso

2Ô∏è‚É£  Fazendo polling do status...
   [     0s | Tentativa  1] Status:    pending | Progresso:   0%
   [     5s | Tentativa  2] Status: processing | Progresso:   0%
   [    30s | Tentativa  6] Status: processing | Progresso:  25%
   [    60s | Tentativa 12] Status: processing | Progresso:  50%
   [   180s | Tentativa 36] Status: processing | Progresso:  85%
   [   300s | Tentativa 60] Status:  completed | Progresso: 100%

‚úÖ JOB COMPLETADO!
   Tempo total: 330.1s
   Canais extra√≠dos: 50
   Sucesso: true
   Tempo de execu√ß√£o: 330.5s
```

## Passo 3: Verificar Logs (Opcional)

Em outro terminal, acompanhe os logs em tempo real:

```bash
docker-compose logs -f scraper-api
```

Procure por linhas como:
- `Job criado: a24992f9-a314-480d-a691-07f0f58f93e1`
- `Job atualizado: ... -> processing`
- `Job completo: ...`

## Passo 4: Teste Manual com cURL (Opcional)

Se preferir testar manualmente:

### 4.1 Iniciar Job

```bash
curl -X POST http://localhost:8000/api/v1/tubehunt/scrape-channels/start
```

Copie o `job_id` do resultado.

### 4.2 Verificar Status (Repetir v√°rias vezes)

```bash
curl http://localhost:8000/api/v1/tubehunt/scrape-channels/result/a24992f9-a314-480d-a691-07f0f58f93e1
```

**Status Pending** (logo ap√≥s iniciar):
```json
{
  "job_id": "a24992f9-a314-480d-a691-07f0f58f93e1",
  "status": "pending",
  "progress": 0,
  "message": "Job enfileirado",
  "started_at": null
}
```

**Status Processing** (alguns segundos depois):
```json
{
  "job_id": "a24992f9-a314-480d-a691-07f0f58f93e1",
  "status": "processing",
  "progress": 35,
  "message": "Iniciando scraping...",
  "started_at": "2026-01-01T22:00:05.000000"
}
```

**Status Completed** (ap√≥s 5-10 minutos):
```json
{
  "job_id": "a24992f9-a314-480d-a691-07f0f58f93e1",
  "status": "completed",
  "result": {
    "success": true,
    "channels": [...],
    "total_channels": 50,
    "timestamp": "2026-01-01T22:05:30.000000",
    "error": null
  },
  "execution_time_seconds": 330.5,
  "completed_at": "2026-01-01T22:05:30.000000"
}
```

## Passo 5: Monitorar M√∫ltiplos Jobs (Opcional)

Voc√™ pode iniciar m√∫ltiplos jobs em paralelo:

```bash
# Terminal 1
curl -X POST http://localhost:8000/api/v1/tubehunt/scrape-channels/start

# Terminal 2
curl -X POST http://localhost:8000/api/v1/tubehunt/scrape-channels/start

# Terminal 3
curl -X POST http://localhost:8000/api/v1/tubehunt/scrape-channels/start
```

A API suporta at√© 5 jobs simult√¢neos.

## ‚úÖ Checklist de Valida√ß√£o

Ap√≥s completar os testes, marque:

- [ ] Docker Compose est√° rodando (2 containers)
- [ ] `python test_job_queue_quick.py` executa sem erros
- [ ] POST retorna em < 100ms
- [ ] GET retorna status "pending" primeiro
- [ ] GET retorna status "processing" segundos depois
- [ ] GET retorna status "completed" ap√≥s 5-10 minutos
- [ ] Resultado cont√©m canais extra√≠dos
- [ ] Logs mostram mensagens de job criado, processando, completo
- [ ] API n√£o bloqueia durante scraping
- [ ] Posso fazer polling enquanto job est√° processando

Se todos os itens foram marcados ‚úÖ, est√° pronto para deploy!

## Pr√≥ximo Passo: Deploy em Produ√ß√£o

Ap√≥s validar localmente:

```bash
git push origin main
```

Render detectar√° automaticamente e far√° deploy da nova vers√£o com o sistema ass√≠ncrono.

## Troubleshooting R√°pido

### Problema: "Connection refused"
```bash
docker-compose up -d
```

### Problema: Script trava indefinidamente
Verifique credenciais no `.env`:
```bash
cat .env | grep -E "url_login|user|password"
```

### Problema: Job fica "processing" por >15 minutos
Verifique logs:
```bash
docker-compose logs scraper-api | tail -50
```

### Problema: Job falha com erro de credenciais
Atualize `.env` com credenciais TubeHunt v√°lidas

## Documenta√ß√£o Completa

Para informa√ß√µes detalhadas, consulte:

- **LOCAL_TESTING_QUICK_START.md** - Refer√™ncia r√°pida
- **ASYNC_TESTING_GUIDE.md** - Guia completo com todos os detalhes
- **TESTING_SUMMARY.md** - Overview t√©cnico da implementa√ß√£o

## Perguntas Frequentes

### P: Quanto tempo leva o scraping?
R: T√≠picamente 5-10 minutos dependendo de quantos canais est√£o no site.

### P: O que acontece se interromper o script?
R: O job continua rodando em background. Use o job_id para consultar status depois.

### P: Posso fazer m√∫ltiplos requests simultaneamente?
R: Sim! A API suporta at√© 5 jobs em paralelo.

### P: O resultado √© armazenado em banco de dados?
R: N√£o, em mem√≥ria. Ser√° perdido se API reiniciar. Para Fase 3, podemos adicionar persist√™ncia.

### P: Como integrar com n8n?
R: Use o job_id para fazer polling no workflow. Ver **ASYNC_TESTING_GUIDE.md** para exemplo.

---

**D√∫vidas?** Consulte a documenta√ß√£o completa ou os arquivos de exemplo de teste.

**Pr√≥ximo passo**: Executar `python test_job_queue_quick.py` e validar que est√° tudo funcionando! üöÄ
